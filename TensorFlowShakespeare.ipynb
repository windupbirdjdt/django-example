{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO94j70u7OoZwMwUam1CP5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windupbirdjdt/django-example/blob/main/TensorFlowShakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRWeN0NMMWv7",
        "outputId": "ea48975e-36c9-4667-a36e-512ebf3425f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "Length of text: 5445609 characters\n",
            "--------\n",
            "The first text in Jose file is \n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose mi\n",
            "--------\n",
            "The first text in TF file is First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# this notebook attempts to train a basic RNN to generate sentences based on purely taking shakespeare as input, and tensorflow tutorial\n",
        "# https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "\n",
        "# data is stored in tensorflow\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text_tf = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text_tf)} characters')\n",
        "\n",
        "# you also have your data set from Jose P\n",
        "alternative_path ='/content/shakespeare.txt'\n",
        "text_jose =  open(alternative_path, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text_jose)} characters')\n",
        "\n",
        "# ok so the Jose P data set is significantly bigger\n",
        "print('--------')\n",
        "print(f'The first text in Jose file is {text_jose[0:100]}')\n",
        "print('--------')\n",
        "print(f'The first text in TF file is {text_tf[0:100]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now start to set up the vocab\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text_jose))\n",
        "print(f'{len(vocab)} unique characters in jose data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnBG5osHOQkG",
        "outputId": "6ea6cdb3-adab-449d-c851-fe0bf4d54936"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84 unique characters in jose data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now start to tokenize the vocab list into ids\n",
        "# convert vocab into ids\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "ALqvhpZyOQwc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can play around with arbitrary strings\n",
        "chars_jt = tf.strings.unicode_split('hello', input_encoding='UTF-8')\n",
        "ids = ids_from_chars(chars_jt)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWzp0y6LOQyq",
        "outputId": "1930b5ab-3cd9-4bad-c764-7bfeb6d7dd45"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([64, 61, 68, 68, 71])>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now convert back\n",
        "chars_jt = chars_from_ids(ids)\n",
        "chars_jt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wp0ufmKOQ1G",
        "outputId": "b132f93d-5d8b-49eb-c2d7-de262cfa621d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'h', b'e', b'l', b'l', b'o'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so you convert text to ids, then use the tensor slices to get the ids\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text_jose, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ],
      "metadata": {
        "id": "vWM3uyHxYB0L"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so can convert these slices back to characters\n",
        "for ids in ids_dataset.take(40):\n",
        "    print(chars_from_ids(ids).numpy().decode('UTF-8'))"
      ],
      "metadata": {
        "id": "W8t20HG7XESi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose an arbitarry length for the sequence, say 100\n",
        "seq_length = 100"
      ],
      "metadata": {
        "id": "MZdYkJL8XxqW"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so now convert the datasets into the sequences\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# lets have a look at the first sequence as an example\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InYs9x7fYM4u",
        "outputId": "8ffb7a0b-98d1-4837-8352-94004983a6a3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
            " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'1' b'\\n' b' ' b' ' b'F' b'r'\n",
            " b'o' b'm' b' ' b'f' b'a' b'i' b'r' b'e' b's' b't' b' ' b'c' b'r' b'e'\n",
            " b'a' b't' b'u' b'r' b'e' b's' b' ' b'w' b'e' b' ' b'd' b'e' b's' b'i'\n",
            " b'r' b'e' b' ' b'i' b'n' b'c' b'r' b'e' b'a' b's' b'e' b',' b'\\n' b' '\n",
            " b' ' b'T' b'h' b'a' b't' b' ' b't' b'h' b'e' b'r' b'e' b'b' b'y' b' '\n",
            " b'b' b'e' b'a' b'u' b't' b'y' b\"'\" b's' b' ' b'r' b'o' b's' b'e' b' '\n",
            " b'm' b'i' b'g'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can define a function to retrive teh text back from characters (using the reduce join function in TF)\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "# and we can use this function to return example \n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUr71VIFfYp0",
        "outputId": "160dd949-071f-479a-d8a8-96a1cb4926a7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mig\"\n",
            "b'ht never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  B'\n",
            "b\"ut thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n\"\n",
            "b'  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that '\n",
            "b\"art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud bur\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to set up source target pairs for the supervised learning whihc will be the sequences shifted by 1 characters\n",
        "# eg if we have sequence Hello, the source is 'Hell', and the output is 'ello'\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n"
      ],
      "metadata": {
        "id": "K8BWvDnSgjwC"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so an example of how this works\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjzH1WIej8Uj",
        "outputId": "d5093ab7-9cea-422d-c353-1e139952767a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so create the data set with the input and target set\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "K8qr9t1VmUd4"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so show this as an example\n",
        "for input_example, target_example in dataset.take(2):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zcmOgzSmn4Q",
        "outputId": "321a64e3-6fa3-4efd-80ec-87435f67dcbd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\"\n",
            "Target: b\"                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mig\"\n",
            "Input : b'ht never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  '\n",
            "Target: b't never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  B'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size (technical thing to help with training)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5df_HXOJmoQk",
        "outputId": "77237bbd-8566-4bca-efc8-ad3706be4d92"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now start to build the model\n",
        "\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "uxvJrjyenrDi"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now set up a class that will essentially define the parameters of the model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YHgvWrxKnrcz"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so now create instance of that class\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# set loss to return from Logits\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# now compile the model with the optimizer function\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "bIrVYw-VnwCz"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as an example show the outputs of teh predictions\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH-xEJgdpCNp",
        "outputId": "8ebaa846-8099-4234-98cf-dcd51084a573"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 85) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this shows an example of the predictions batch\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices\n",
        "\n",
        "# show example prediction on the untrained model\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnHgreOpNcb",
        "outputId": "c7060666-1f67-4c09-8a61-65829438e2e9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'es be but young and fair,\\n    They have the gift to know it; and in his brain,\\n    Which is as dry a'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'pEU?PETXGK8QW58J7QbPXm)<u6ijrn1UH[UNK]anW}!WRq5SkN!1mD3bX.[UNK]kRRKzh37>KEVwj>rOUlV&&?Hjtqx&<4lW)u,]tuS[3->9'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  now train the model\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZV9yDawCpWsX"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the number of epochs\n",
        "\n",
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "0Pe3h4XLpvFV"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now actually train it\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adTBqsSLpwwJ",
        "outputId": "fce7ef30-a717-4ed8-f89e-9350308a3047"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "842/842 [==============================] - 17s 14ms/step - loss: 1.9067\n",
            "Epoch 2/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.3231\n",
            "Epoch 3/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.2205\n",
            "Epoch 4/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.1702\n",
            "Epoch 5/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.1353\n",
            "Epoch 6/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.1081\n",
            "Epoch 7/30\n",
            "842/842 [==============================] - 10s 10ms/step - loss: 1.0849\n",
            "Epoch 8/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0647\n",
            "Epoch 9/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 1.0470\n",
            "Epoch 10/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0308\n",
            "Epoch 11/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0171\n",
            "Epoch 12/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0055\n",
            "Epoch 13/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 0.9959\n",
            "Epoch 14/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 0.9877\n",
            "Epoch 15/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9825\n",
            "Epoch 16/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9778\n",
            "Epoch 17/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9750\n",
            "Epoch 18/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9762\n",
            "Epoch 19/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9801\n",
            "Epoch 20/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 0.9757\n",
            "Epoch 21/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 0.9794\n",
            "Epoch 22/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0167\n",
            "Epoch 23/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 0.9909\n",
            "Epoch 24/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0302\n",
            "Epoch 25/30\n",
            "842/842 [==============================] - 9s 10ms/step - loss: 1.0707\n",
            "Epoch 26/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0177\n",
            "Epoch 27/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0116\n",
            "Epoch 28/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0205\n",
            "Epoch 29/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0284\n",
            "Epoch 30/30\n",
            "842/842 [==============================] - 9s 9ms/step - loss: 1.0447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below code makes a prediction!!\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "f4MpLpSIp5dn"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "AqD7NOkyqlAw"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall this works well output example pasted below. learns words but not meaning\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(2000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ73oOf8qoZx",
        "outputId": "e918337e-25af-4978-fc2e-94ade83e0692"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: PROTEUTUS hallow-day, to give, and made me cry\n",
            "               here. What half my sword, Clifford.\n",
            "  QUICKLY. Most true, and that which I look to't.\n",
            "  OLIVIA. Hast thou no other dear love?\n",
            "  THIRD SERVANT. I desire you this.\n",
            "  MALVOLIO. Kneater, good Comparise within. You jog and a cause to-night,\n",
            "    I know not why, which thou forget'st to report.  \n",
            "    'Tis near a troution. Let me seeet you were\n",
            "    Herself, does pieces. Cassion's burial fight.\n",
            "    A direct gods they thus must prove retreat her.\n",
            "  CLOWN. O Fool were I! Was it was to horse!\n",
            "  DUKE. Not you, sir,\n",
            "    You would entreat them descend; and then to Post\n",
            "    Whose couple is not passionate Pompe and Troilus' shits\n",
            "    That swear I'll strudge their topriguous lost.\n",
            "\n",
            "                      Re-enter PYRAMUS mine eyes obscury,\n",
            "                                     and others\n",
            "  ADAM, Nurse, Orleans, TOUCHSTONE and GENTLEMEN, how fleet\n",
            "  PATLE CLIFF and Verges' flict of speaking fathful, for\n",
            "          and OFFICERSHE FOR YOUR\n",
            "    Give her masquier.\n",
            "  AGUECHEEK. And you of the rest good, at your presence nice a\n",
            "    Pendow, lord Angelo, I am sure you love what 's thou\n",
            "    saw her, to curtsire away. The conduit, I take thee she were a\n",
            "    perioding thy plight should in; and it was already abused,\n",
            "    but thou shalt hust? Why, for your eye-sweet musicion.\n",
            "    To call thee in the work of him.\n",
            "                             [A level rite but not of it.  Sir Viceoneby.\n",
            "\n",
            "  Leon. Curs'd be thy lord, with hit painting in my mouth- that your money\n",
            "    is too ungrinkisd for life?\n",
            "  PAGE. Yea, and pass not him to me, I will seek thee hence.\n",
            "  PROTEUS. Good for your crush, give nine special court.\n",
            "  PEDANT. Good morrow, rain!\n",
            "  IAGO. I do believe that I am not a fool;\n",
            "    In thy orifect esteeming that I would\n",
            "    have sent in my huel abroad-cueks to the voice. 'a would speak with you!\n",
            "    Take this give black my wit, thy loving worth,\n",
            "    Thou art the fogla's trenched heel of writting\n",
            "    A lily wrong in thy affliction truth.\n",
            "  LE \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.9659790992736816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg5rxwvbsvgL",
        "outputId": "744f8866-5a72-4ad2-e2b2-4c08f7344cd4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f70deb88550>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### output shown below\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "hjndMePls5nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROMEO: PROTEUTUS hallow-day, to give, and made me cry\n",
        "               here. What half my sword, Clifford.\n",
        "  QUICKLY. Most true, and that which I look to't.\n",
        "  OLIVIA. Hast thou no other dear love?\n",
        "  THIRD SERVANT. I desire you this.\n",
        "  MALVOLIO. Kneater, good Comparise within. You jog and a cause to-night,\n",
        "    I know not why, which thou forget'st to report.  \n",
        "    'Tis near a troution. Let me seeet you were\n",
        "    Herself, does pieces. Cassion's burial fight.\n",
        "    A direct gods they thus must prove retreat her.\n",
        "  CLOWN. O Fool were I! Was it was to horse!\n",
        "  DUKE. Not you, sir,\n",
        "    You would entreat them descend; and then to Post\n",
        "    Whose couple is not passionate Pompe and Troilus' shits\n",
        "    That swear I'll strudge their topriguous lost.\n",
        "\n",
        "                      Re-enter PYRAMUS mine eyes obscury,\n",
        "                                     and others\n",
        "  ADAM, Nurse, Orleans, TOUCHSTONE and GENTLEMEN, how fleet\n",
        "  PATLE CLIFF and Verges' flict of speaking fathful, for\n",
        "          and OFFICERSHE FOR YOUR\n",
        "    Give her masquier.\n",
        "  AGUECHEEK. And you of the rest good, at your presence nice a\n",
        "    Pendow, lord Angelo, I am sure you love what 's thou\n",
        "    saw her, to curtsire away. The conduit, I take thee she were a\n",
        "    perioding thy plight should in; and it was already abused,\n",
        "    but thou shalt hust? Why, for your eye-sweet musicion.\n",
        "    To call thee in the work of him.\n",
        "                             [A level rite but not of it.  Sir Viceoneby.\n",
        "\n",
        "  Leon. Curs'd be thy lord, with hit painting in my mouth- that your money\n",
        "    is too ungrinkisd for life?\n",
        "  PAGE. Yea, and pass not him to me, I will seek thee hence.\n",
        "  PROTEUS. Good for your crush, give nine special court.\n",
        "  PEDANT. Good morrow, rain!\n",
        "  IAGO. I do believe that I am not a fool;\n",
        "    In thy orifect esteeming that I would\n",
        "    have sent in my huel abroad-cueks to the voice. 'a would speak with you!\n",
        "    Take this give black my wit, thy loving worth,\n",
        "    Thou art the fogla's trenched heel of writting\n",
        "    A lily wrong in thy affliction truth.\n",
        "  LE "
      ],
      "metadata": {
        "id": "6nv-LWHavNUF"
      }
    }
  ]
}